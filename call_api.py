import os
import base64
import json
import multiprocessing
import time
from pathlib import Path
from multiprocessing import Pool, Manager
from openai import OpenAI


def load_config(config_path="config.json"):
    """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°")
        raise
    except json.JSONDecodeError:
        print(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ {config_path} æ ¼å¼ä¸æ­£ç¡®")
        raise


def encode_image_to_base64(image_path):
    """å°†æœ¬åœ°å›¾åƒè½¬æ¢ä¸ºbase64ç¼–ç """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def analyze_single_image(args):
    """
    å•ä¸ªå›¾ç‰‡åˆ†æä»»åŠ¡ - è¿›ç¨‹ç‰ˆæœ¬
    å‚æ•°: (image_path, prompt, api_key, base_url, model_type, process_id, output_dir)
    """
    image_path, prompt, api_key, base_url, model_type, process_id, output_dir = args

    # è·å–è¿›ç¨‹ID
    current_process = multiprocessing.current_process()
    pid = current_process.pid

    # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)
    output_file = output_dir / f"process_{process_id}_output.txt"

    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºè¿›åº¦
    print(f"âš¡ [è¿›ç¨‹{process_id} PID:{pid}] å¼€å§‹åˆ†æ: {image_path.name}")

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = OpenAI(api_key=api_key, base_url=base_url)

    # å°†å›¾åƒè½¬æ¢ä¸º base64
    base64_image = encode_image_to_base64(image_path)

    try:
        # å‘è¾“å‡ºæ–‡ä»¶å†™å…¥å¼€å§‹ä¿¡æ¯
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(f"\n{'=' * 60}\n")
            f.write(f"ğŸ–¼ï¸ å›¾ç‰‡: {image_path.name}\n")
            f.write(f"âš¡ è¿›ç¨‹: {process_id} (PID: {pid})\n")
            f.write(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'=' * 60}\n")

        # å‘æ¨¡å‹å‘é€è¯·æ±‚
        response = client.chat.completions.create(
            model=model_type,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            },
                        },
                    ],
                }
            ],
            stream=True,
        )

        # æ”¶é›†å“åº”å†…å®¹ï¼Œåªå†™å…¥æ–‡ä»¶ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°
        full_response = ""
        for chunk in response:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if delta.content:
                content = delta.content
                full_response += content
                # å†™å…¥æ–‡ä»¶
                with open(output_file, 'a', encoding='utf-8') as f:
                    f.write(content)
            if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                reasoning = delta.reasoning_content
                full_response += reasoning
                # å†™å…¥æ–‡ä»¶
                with open(output_file, 'a', encoding='utf-8') as f:
                    f.write(reasoning)

        # å†™å…¥ç»“æŸä¿¡æ¯
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(f"\n\nâœ… åˆ†æå®Œæˆ\n")
            f.write(f"ğŸ“ æ€»å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦\n")
            f.write(f"â° ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'=' * 60}\n\n")

        print(f"âœ… [è¿›ç¨‹{process_id} PID:{pid}] å®Œæˆ: {image_path.name}")

        return {
            "image_name": image_path.name,
            "process_id": process_id,
            "pid": pid,
            "response": full_response,
            "status": "success",
            "response_length": len(full_response),
            "output_file": str(output_file)
        }

    except Exception as e:
        # å†™å…¥é”™è¯¯ä¿¡æ¯
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(f"\nâŒ åˆ†æå¤±è´¥\n")
            f.write(f"é”™è¯¯ä¿¡æ¯: {str(e)}\n")
            f.write(f"â° é”™è¯¯æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'=' * 60}\n\n")

        print(f"âŒ [è¿›ç¨‹{process_id} PID:{pid}] åˆ†æå¤±è´¥: {image_path.name} - {e}")
        return {
            "image_name": image_path.name,
            "process_id": process_id,
            "pid": pid,
            "response": "",
            "status": f"error: {str(e)}",
            "response_length": 0,
            "output_file": str(output_file)
        }


def analyze_skin_images_multiprocess(config_path="config.json"):
    """
    å¤šè¿›ç¨‹æ‰¹é‡åˆ†ææ–‡ä»¶å¤¹ä¸­çš„çš®è‚¤å›¾åƒ
    """
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    api_config = config["api_config"]
    analysis_config = config["analysis_config"]

    # è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    folder_path = analysis_config["folder_path"]
    supported_formats = analysis_config["supported_formats"]

    image_files = []
    for f in os.listdir(folder_path):
        file_path = Path(folder_path) / f
        if file_path.suffix.lower() in supported_formats and file_path.is_file():
            image_files.append(file_path)

    if not image_files:
        print("æœªåœ¨è¯¥æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°å›¾ç‰‡ã€‚")
        return []

    print(f"å…±æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
    print(f"ä½¿ç”¨ {len(api_config['api_keys'])} ä¸ªAPIå¯†é’¥è¿›è¡Œå¤šè¿›ç¨‹åˆ†æ...")
    print(f"å¤§æ¨¡å‹è¾“å‡ºå°†ä¿å­˜åˆ°ç‹¬ç«‹çš„txtæ–‡ä»¶ä¸­...\n")

    # å‡†å¤‡ä»»åŠ¡å‚æ•°
    tasks = []
    api_keys = api_config["api_keys"]
    output_dir = "process_outputs"

    for i, image_path in enumerate(image_files):
        # è½®è¯¢åˆ†é…APIå¯†é’¥
        api_key = api_keys[i % len(api_keys)]
        process_id = i % len(api_keys) + 1  # è¿›ç¨‹IDä»1å¼€å§‹

        task_args = (
            image_path,
            analysis_config["prompt"],
            api_key,
            api_config["base_url"],
            api_config["model_type"],
            process_id,
            output_dir
        )
        tasks.append(task_args)

    # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œä»»åŠ¡
    max_workers = min(analysis_config.get("max_workers", 16), len(api_keys), 16)
    results = []

    print(f"å¯åŠ¨ {max_workers} ä¸ªè¿›ç¨‹è¿›è¡Œåˆ†æ...\n")
    start_time = time.time()

    # ä½¿ç”¨è¿›ç¨‹æ± 
    with Pool(processes=max_workers) as pool:
        results = pool.map(analyze_single_image, tasks)

    end_time = time.time()

    # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶ä¿¡æ¯
    print(f"\nğŸ“ è¿›ç¨‹è¾“å‡ºæ–‡ä»¶:")
    process_stats = {}
    for process_id in range(1, max_workers + 1):
        output_file = Path(output_dir) / f"process_{process_id}_output.txt"
        if output_file.exists():
            # ç»Ÿè®¡è¯¥è¿›ç¨‹å¤„ç†çš„å›¾ç‰‡æ•°é‡
            process_images = [r for r in results if r["process_id"] == process_id]
            success_count = sum(1 for r in process_images if r["status"] == "success")
            file_size = output_file.stat().st_size
            process_stats[process_id] = {
                "file": output_file,
                "total": len(process_images),
                "success": success_count,
                "size": file_size
            }
            print(f"  è¿›ç¨‹ {process_id}: {output_file}")
            print(f"     å¤„ç†å›¾ç‰‡: {len(process_images)} å¼ , æˆåŠŸ: {success_count} å¼ ")
            print(f"     æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

    # ç»Ÿè®¡æ€»ä½“ç»“æœ
    success_count = sum(1 for r in results if r["status"] == "success")
    error_count = len(results) - success_count

    print(f"\nğŸ‰ æ‰€æœ‰å›¾ç‰‡åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±å¤„ç†: {len(results)} å¼ å›¾ç‰‡")
    print(f"âœ… æˆåŠŸ: {success_count} å¼ ")
    print(f"âŒ å¤±è´¥: {error_count} å¼ ")
    print(f"â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(results) / (end_time - start_time):.2f} å›¾ç‰‡/ç§’")

    return results, process_stats


def create_summary_file(results, process_stats, output_dir="process_outputs"):
    """åˆ›å»ºæ±‡æ€»æ–‡ä»¶"""
    summary_path = Path(output_dir) / "process_summary.txt"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("å¤šè¿›ç¨‹çš®è‚¤å›¾åƒåˆ†ææ±‡æ€»\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"æ€»å›¾ç‰‡æ•°é‡: {len(results)}\n")
        f.write(f"ä½¿ç”¨è¿›ç¨‹æ•°é‡: {len(process_stats)}\n")
        f.write(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # æŒ‰è¿›ç¨‹ç»Ÿè®¡
        f.write("å„è¿›ç¨‹å¤„ç†æƒ…å†µ:\n")
        f.write("-" * 40 + "\n")
        for process_id, stats in sorted(process_stats.items()):
            success_rate = (stats["success"] / stats["total"]) * 100 if stats["total"] > 0 else 0
            f.write(f"è¿›ç¨‹ {process_id}:\n")
            f.write(f"  å¤„ç†å›¾ç‰‡: {stats['success']}/{stats['total']} æˆåŠŸ\n")
            f.write(f"  æˆåŠŸç‡: {success_rate:.1f}%\n")
            f.write(f"  è¾“å‡ºæ–‡ä»¶: {stats['file'].name}\n")
            f.write(f"  æ–‡ä»¶å¤§å°: {stats['size']} å­—èŠ‚\n\n")

        # æ€»ä½“ç»Ÿè®¡
        total_success = sum(stats["success"] for stats in process_stats.values())
        total_rate = (total_success / len(results)) * 100 if results else 0
        f.write(f"æ€»ä½“ç»Ÿè®¡: {total_success}/{len(results)} æˆåŠŸ ({total_rate:.1f}%)\n")

    print(f"ğŸ“‹ è¿›ç¨‹æ±‡æ€»æ–‡ä»¶: {summary_path}")


if __name__ == "__main__":
    # åœ¨Windowsä¸Šä½¿ç”¨å¤šè¿›ç¨‹éœ€è¦è¿™ä¸ªä¿æŠ¤
    multiprocessing.freeze_support()

    # === ä½¿ç”¨å¤šè¿›ç¨‹æ‰§è¡Œæ‰¹é‡åˆ†æ ===
    print("ğŸš€ å¯åŠ¨16è¿›ç¨‹çš®è‚¤å›¾åƒåˆ†æç³»ç»Ÿ...")
    results, process_stats = analyze_skin_images_multiprocess("config.json")

    # åˆ›å»ºæ±‡æ€»æ–‡ä»¶
    if results:
        create_summary_file(results, process_stats)

    print("\nğŸ¯ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")